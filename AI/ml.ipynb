{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6-S2CxoVZKk"
   },
   "source": [
    "# Import libraray and Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "INAVooq1vOdj"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ycCpFpcV3IZ"
   },
   "source": [
    "# Prepare X, y for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvcPfdWevx6L"
   },
   "outputs": [],
   "source": [
    "# count the number of data in each layer\n",
    "# 0(mono), 1(bi), 2(tri), 3(quad), 4(penta), 5(thick)\n",
    "def calculate_feature_lengths(df, index):\n",
    "  lengths = {}\n",
    "  for i in index:\n",
    "    lengths[i] = len(df['data'][0][i]['dist']['RGB_dist'])\n",
    "  return lengths\n",
    "\n",
    "# configure X for machine learning\n",
    "def make_X(df, indices):\n",
    "    data_list = []\n",
    "    for index in indices:\n",
    "        data = df['data'][0][index]['dist']\n",
    "        X_data = []\n",
    "        for key, value in data.items():\n",
    "            X_data.append(np.array(value))\n",
    "        data_list.append(np.column_stack(X_data).T)\n",
    "\n",
    "    return np.concatenate(data_list, axis=1)\n",
    "\n",
    "# configure y for machine learning\n",
    "def make_Y(index_feature_lengths):\n",
    "  y = []\n",
    "  for key, value in index_feature_lengths.items():\n",
    "    for _ in range(value):\n",
    "      y.append(key)\n",
    "  return np.array(y).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file_path = os.getcwd()\n",
    "current_dir = os.path.dirname(current_file_path)\n",
    "input_file_path = os.path.join(current_dir, \"data_preprocess\", \"data\", \"json\", \"Graphene_merge.json\")\n",
    "output_file_dir = os.path.join(current_dir, \"AI\", \"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YG13i1h7uXmw",
    "outputId": "e68e5ce0-7ddd-4a34-b1a7-456e65f03df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "material:Graphene\n",
      "substrate:Gelpack\n",
      "{0: 8633, 1: 2272, 2: 650, 3: 0, 4: 0, 5: 4975}\n"
     ]
    }
   ],
   "source": [
    "# Load data (material, color for confusion matrix)\n",
    "material_and_color = [('Graphene', 'Blues')]\n",
    "idx = 0\n",
    "\n",
    "with open(input_file_path, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# JSON -> Pandas DataFrame\n",
    "df = pd.json_normalize(json_data)\n",
    "\n",
    "# Load material and substrate data\n",
    "MATERIAL = df['material'].to_string(index=False)\n",
    "SUBSTRATE = df['substrate'].to_string(index=False)\n",
    "COLOR = material_and_color[idx][1]\n",
    "print(f'material:{MATERIAL}\\nsubstrate:{SUBSTRATE}')\n",
    "\n",
    "# Load layer data in material\n",
    "index = [0, 1, 2, 3, 4, 5] # mono, bi, tri, quad, penta, thick\n",
    "index_feature_lengths = calculate_feature_lengths(df, index) # count the data according to the number of layer\n",
    "print(index_feature_lengths)\n",
    "\n",
    "# make X, y\n",
    "data_labels = [key for key, item in index_feature_lengths.items() if item > 0]\n",
    "y = make_Y(index_feature_lengths)\n",
    "X = make_X(df, index)\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIJJzD6ywLyK"
   },
   "source": [
    "# Multiple Classifcation\n",
    "\n",
    "v1. train vs valid vs test & valid, test acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90Awfb2DluNT"
   },
   "outputs": [],
   "source": [
    "def machineLearning_print_val_test(feature, X=X, y=y, material=MATERIAL, color=COLOR, labels=data_labels):\n",
    "\n",
    "    # merge confusion matrices in one image file\n",
    "    def combine_images(images):\n",
    "        images = [Image.open(x) for x in images]\n",
    "        widths, heights = zip(*(i.size for i in images))\n",
    "\n",
    "        total_width = sum(widths)\n",
    "        max_height = max(heights)\n",
    "\n",
    "        combined_image = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "        x_offset = 0\n",
    "\n",
    "        for im in images:\n",
    "            combined_image.paste(im, (x_offset, 0))\n",
    "            x_offset += im.size[0]\n",
    "\n",
    "        return combined_image\n",
    "\n",
    "    feature_mapping = {\n",
    "        'rgb': 'RGB', 'yiq': 'YIQ'\n",
    "    }\n",
    "\n",
    "    feature_names = [feature_mapping.get(f, f.upper()) for f in feature]\n",
    "    feature_name = ''.join(feature_names)\n",
    "    \n",
    "    output_file_path_for_conf_mat = os.path.join(output_file_dir, f\"{feature_name}.png\")\n",
    "    output_file_path_for_table = os.path.join(output_file_dir, f\"{feature_name}.csv\")\n",
    "\n",
    "    feature_dict = {\n",
    "        'RGB_dist': 0, 'YIQ_dist': 1\n",
    "    }\n",
    "\n",
    "    feature_indices = [feature_dict[f] for f in feature]\n",
    "    X_selected = X[:, feature_indices]\n",
    "\n",
    "    # Train/Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    layer_dict = {0: 'mono', 1: 'bi', 2:'tri', 3:'quad', 4: 'penta', 5: 'thick'}\n",
    "    layer_labels = [layer_dict[lb] for lb in labels]\n",
    "    layer_numbers = [key for key, value in layer_dict.items() if value in layer_labels]\n",
    "\n",
    "    fold_results_list = []\n",
    "    images = [] \n",
    "\n",
    "    models = {\n",
    "        'SVM': SVC(),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'DecisionTree': DecisionTreeClassifier(),\n",
    "    }\n",
    "\n",
    "    # Cross-validation for each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f'Training {model_name} with 5-Fold Cross Validation...\\n')\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "            \n",
    "            # Split data into train/validation for this fold\n",
    "            X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            # Fit the model on the current fold\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            # Validate on the validation set\n",
    "            y_val_pred = model.predict(X_val_fold)\n",
    "            val_acc = round(accuracy_score(y_val_fold, y_val_pred), 4)\n",
    "\n",
    "            # Test on the test set\n",
    "            y_test_pred = model.predict(X_test)\n",
    "            test_acc = round(accuracy_score(y_test, y_test_pred), 4)\n",
    "\n",
    "            # Precision, Recall, F1-Score \n",
    "            precision = precision_score(y_test, y_test_pred, average=None, labels=layer_numbers, zero_division=0)\n",
    "            recall = recall_score(y_test, y_test_pred, average=None, labels=layer_numbers)\n",
    "            f1 = f1_score(y_test, y_test_pred, average=None, labels=layer_numbers)\n",
    "\n",
    "            fold_results_list.append({\n",
    "                'Model': model_name,\n",
    "                'Fold': fold + 1,\n",
    "                'Validation Accuracy': val_acc,\n",
    "                'Test Accuracy': test_acc,\n",
    "                'Precision': [round(p, 2) for p in precision],\n",
    "                'Recall': [round(r, 2) for r in recall],\n",
    "                'F1-Score': [round(f, 2) for f in f1]\n",
    "            })\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        sns.set(font_scale=2.0)\n",
    "\n",
    "        heatmap = sns.heatmap(conf_matrix, annot=True, cmap=color, fmt='d', cbar=False,\n",
    "                              xticklabels=layer_labels, yticklabels=layer_labels, annot_kws={\"size\": 36})\n",
    "\n",
    "        plt.title(model_name, size=48)\n",
    "        plt.xlabel('Predicted', size=24)\n",
    "        plt.ylabel('True', size=24)\n",
    "\n",
    "        buf = BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        plt.close()\n",
    "        buf.seek(0)\n",
    "        images.append(buf)\n",
    "\n",
    "    # Write report in CSV file\n",
    "    fold_results_df = pd.DataFrame(fold_results_list)\n",
    "    fold_results_df.to_csv(output_file_path_for_table, index=False)\n",
    "\n",
    "    # Combine and save images\n",
    "    combined_image = combine_images(images)\n",
    "    combined_image.save(output_file_path_for_conf_mat)\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRlrtSni006G",
    "outputId": "ca4a08a8-8c42-4fc1-cbaa-adc6749fd0c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with 5-Fold Cross Validation...\n",
      "\n",
      "Training KNN with 5-Fold Cross Validation...\n",
      "\n",
      "Training DecisionTree with 5-Fold Cross Validation...\n",
      "\n",
      "Training SVM with 5-Fold Cross Validation...\n",
      "\n",
      "Training KNN with 5-Fold Cross Validation...\n",
      "\n",
      "Training DecisionTree with 5-Fold Cross Validation...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "machineLearning_print_val_test(['RGB_dist'])\n",
    "machineLearning_print_val_test(['YIQ_dist'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
